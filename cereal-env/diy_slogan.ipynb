{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIY Cereal Slogans \n",
    "\n",
    "Use a RNN (Recurrent Neural Network) to generate a cereal slogan! This program is based on an adaptation of the information and tutorial described in: https://thepythoncode.com/article/text-generation-keras-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "import random\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = './data/just_slogans.txt'\n",
    "BASENAME = os.path.basename(FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 5528\n"
     ]
    }
   ],
   "source": [
    "# get data and prep it, remove case\n",
    "text = open(FILE_PATH, encoding='utf-8').read().lower()\n",
    "\n",
    "# remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", punctuation))\n",
    "\n",
    "print(f\"text length: {len(text)}\")\n",
    "\n",
    "# create a mapping \n",
    "unique_chars = sorted(list(set(text)))\n",
    "\n",
    "# number of unique chars \n",
    "num_unique = len(unique_chars)\n",
    "\n",
    "# char -> int\n",
    "char_int = {c: i for i, c in enumerate(unique_chars)}\n",
    "\n",
    "# int -> char\n",
    "int_char = {i: c for i, c in enumerate(unique_chars)}\n",
    "\n",
    "# convert all of the text into ints\n",
    "encoded_text = np.array([char_int[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dicts for text gen \n",
    "pickle.dump(char_int, open(f\"./data/{BASENAME}-char_int.pickle\", \"wb\"))\n",
    "pickle.dump(int_char, open(f\"./data/{BASENAME}-int_char.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "subsequence_length = 100\n",
    "batch_size = 128 # memory can read in efficiently if its in bytes\n",
    "epochs = 30\n",
    "\n",
    "# create a dataset object for efficient handling \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
    "\n",
    "# # print characters and their integer representations \n",
    "# for char in char_dataset.take(10):\n",
    "#     print(char.numpy(), int_char[char.numpy()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the Dataset \n",
    "- split into inputs and targets to expand the dataset\n",
    "- one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a bowl a day keeps the bullies away\n",
      "a dish that’s a winner for wartime\n",
      "a is for apple j is for jacks cinnamon toasty applejacks\n",
      "alphabits you know you want them come and have some\n",
      "always after my lucky\n",
      " charms they’re magically delicious\n",
      "applejacks will not be sold to bullies\n",
      "applejacks where the sweet taste of cinnamon is the winnamon\n",
      "ask for them by name\n",
      "because that’s the kind of mom you are\n",
      "bet y\n"
     ]
    }
   ],
   "source": [
    "# build sequences to be the input of the text generation, the output will be a single character that's predicted \n",
    "sequences = char_dataset.batch(2 * subsequence_length + 1, drop_remainder=True)\n",
    "\n",
    "# this is an example of two sequences that will be fed in\n",
    "for sequence in sequences.take(2):\n",
    "    print(''.join([int_char[i] for i in sequence.numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a sequence into input, target samples \n",
    "def split_sample(sample):\n",
    "\n",
    "    # basically taking a sequence and putting it into (input, target format)\n",
    "    # ex. subsequence_length = 5, sequence: theyre grrrreat\n",
    "    # (input, target) -> (theyre grr, r)\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensors((sample[:subsequence_length], sample[subsequence_length]))\n",
    "    \n",
    "    # repeat this by splitting the sequences even further, going character by character \n",
    "    for i in range(1, (len(sample) - 1) // 2):\n",
    "        input_ = sample[i: i + subsequence_length]\n",
    "        target = sample[i + subsequence_length]\n",
    "\n",
    "        # create a larger dataset by converting into (input, target)\n",
    "        other_ds = tf.data.Dataset.from_tensors((input_, target))\n",
    "        ds = ds.concatenate(other_ds)\n",
    "    return ds\n",
    "\n",
    "# create inputs and targets\n",
    "dataset = sequences.flat_map(split_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding for the samples \n",
    "def one_hot_samples(input_, target):\n",
    "    return tf.one_hot(input_, num_unique), tf.one_hot(target, num_unique)\n",
    "\n",
    "dataset = dataset.map(one_hot_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: a bowl a day keeps the bullies away\n",
      "a dish that’s a winner for wartime\n",
      "a is for apple j is for jacks\n",
      "Target:  \n",
      "Input shape: (100, 32)\n",
      "Target shape: (32,)\n",
      "================================================== \n",
      "\n",
      "Input:  bowl a day keeps the bullies away\n",
      "a dish that’s a winner for wartime\n",
      "a is for apple j is for jacks \n",
      "Target: c\n",
      "Input shape: (100, 32)\n",
      "Target shape: (32,)\n",
      "================================================== \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print first 2 samples\n",
    "for element in dataset.take(2):\n",
    "    print(\"Input:\", ''.join([int_char[np.argmax(char_vector)] for char_vector in element[0].numpy()]))\n",
    "    print(\"Target:\", int_char[np.argmax(element[1].numpy())])\n",
    "    print(\"Input shape:\", element[0].shape)\n",
    "    print(\"Target shape:\", element[1].shape)\n",
    "    print(\"=\"*50, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle and split dataset into batches \n",
    "prepped_dataset = dataset.repeat().shuffle(1024).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Model \n",
    "- LSTM Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100, 256)          295936    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 256)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 256)               525312    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                8224      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 829472 (3.16 MB)\n",
      "Trainable params: 829472 (3.16 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(subsequence_length, num_unique), return_sequences=True), \n",
    "    Dropout(0.3), \n",
    "    LSTM(256), \n",
    "    Dense(num_unique, activation=\"softmax\")])\n",
    "\n",
    "# define model path \n",
    "model_weights_path = f\"results/{BASENAME}-{subsequence_length}.h5\"\n",
    "model.summary()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\", \"recall\", \"precision\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "42/42 [==============================] - 39s 837ms/step - loss: 3.0699 - accuracy: 0.1423\n",
      "Epoch 2/30\n",
      "42/42 [==============================] - 40s 946ms/step - loss: 2.9662 - accuracy: 0.1659\n",
      "Epoch 3/30\n",
      "42/42 [==============================] - 43s 1s/step - loss: 2.9560 - accuracy: 0.1596\n",
      "Epoch 4/30\n",
      "42/42 [==============================] - 41s 985ms/step - loss: 2.9214 - accuracy: 0.1682\n",
      "Epoch 5/30\n",
      "42/42 [==============================] - 38s 898ms/step - loss: 2.8593 - accuracy: 0.1838\n",
      "Epoch 6/30\n",
      "42/42 [==============================] - 39s 923ms/step - loss: 2.7340 - accuracy: 0.2310\n",
      "Epoch 7/30\n",
      "42/42 [==============================] - 40s 956ms/step - loss: 2.6069 - accuracy: 0.2461\n",
      "Epoch 8/30\n",
      "42/42 [==============================] - 41s 976ms/step - loss: 2.4897 - accuracy: 0.2770\n",
      "Epoch 9/30\n",
      "42/42 [==============================] - 41s 978ms/step - loss: 2.3475 - accuracy: 0.3110\n",
      "Epoch 10/30\n",
      "42/42 [==============================] - 41s 986ms/step - loss: 2.2253 - accuracy: 0.3398\n",
      "Epoch 11/30\n",
      "42/42 [==============================] - 42s 1s/step - loss: 2.1131 - accuracy: 0.3659\n",
      "Epoch 12/30\n",
      "42/42 [==============================] - 41s 970ms/step - loss: 1.9739 - accuracy: 0.4007\n",
      "Epoch 13/30\n",
      "42/42 [==============================] - 42s 991ms/step - loss: 1.8467 - accuracy: 0.4407\n",
      "Epoch 14/30\n",
      "42/42 [==============================] - 41s 988ms/step - loss: 1.6516 - accuracy: 0.4963\n",
      "Epoch 15/30\n",
      "42/42 [==============================] - 41s 977ms/step - loss: 1.4894 - accuracy: 0.5463\n",
      "Epoch 16/30\n",
      "42/42 [==============================] - 41s 989ms/step - loss: 1.3030 - accuracy: 0.6125\n",
      "Epoch 17/30\n",
      "42/42 [==============================] - 41s 979ms/step - loss: 1.1151 - accuracy: 0.6775\n",
      "Epoch 18/30\n",
      "42/42 [==============================] - 41s 973ms/step - loss: 0.9293 - accuracy: 0.7374\n",
      "Epoch 19/30\n",
      "42/42 [==============================] - 41s 982ms/step - loss: 0.7400 - accuracy: 0.8131\n",
      "Epoch 20/30\n",
      "42/42 [==============================] - 42s 989ms/step - loss: 0.6007 - accuracy: 0.8635\n",
      "Epoch 21/30\n",
      "42/42 [==============================] - 44s 1s/step - loss: 0.4803 - accuracy: 0.9016\n",
      "Epoch 22/30\n",
      "42/42 [==============================] - 41s 986ms/step - loss: 0.3669 - accuracy: 0.9410\n",
      "Epoch 23/30\n",
      "42/42 [==============================] - 42s 990ms/step - loss: 0.2812 - accuracy: 0.9671\n",
      "Epoch 24/30\n",
      "42/42 [==============================] - 41s 982ms/step - loss: 0.2155 - accuracy: 0.9829\n",
      "Epoch 25/30\n",
      "42/42 [==============================] - 42s 991ms/step - loss: 0.1685 - accuracy: 0.9896\n",
      "Epoch 26/30\n",
      "42/42 [==============================] - 46s 1s/step - loss: 0.1315 - accuracy: 0.9944\n",
      "Epoch 27/30\n",
      "42/42 [==============================] - 41s 980ms/step - loss: 0.1046 - accuracy: 0.9974\n",
      "Epoch 28/30\n",
      "42/42 [==============================] - 41s 971ms/step - loss: 0.0858 - accuracy: 0.9993\n",
      "Epoch 29/30\n",
      "42/42 [==============================] - 41s 978ms/step - loss: 0.0701 - accuracy: 0.9983\n",
      "Epoch 30/30\n",
      "42/42 [==============================] - 42s 995ms/step - loss: 0.0624 - accuracy: 0.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ram19\\Documents\\Fun\\cereal-mascotter\\cereal-env\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model.fit(prepped_dataset, steps_per_epoch=(len(encoded_text) - subsequence_length) // batch_size, epochs=epochs)\n",
    "\n",
    "# save it!\n",
    "model.save(model_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load vocab dictionaries\n",
    "char_int = pickle.load(open(f\"./data/{BASENAME}-char_int.pickle\", \"rb\"))\n",
    "int_char = pickle.load(open(f\"./data/{BASENAME}-int_char.pickle\", \"rb\"))\n",
    "vocab_size = len(char_int)\n",
    "\n",
    "# build model again \n",
    "# building the model\n",
    "model = Sequential([\n",
    "    LSTM(256, input_shape=(subsequence_length, vocab_size), return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    LSTM(256),\n",
    "    Dense(vocab_size, activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "# load the optimal weights\n",
    "model.load_weights(f\"results/{BASENAME}-{subsequence_length}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = random.randint(0, len(text) - 90 - 1)\n",
    "generated = ''\n",
    "seed = text[start_index: start_index + 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating text: 100%|██████████| 30/30 [00:02<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: oney nut cheerios\n",
      "what are you eating nutin’ honey\n",
      "what’s the good word bird\n",
      "what’s new li\n",
      "\n",
      "Generated Text: me an hor taste that’s of lins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "s = seed\n",
    "n_chars = 30\n",
    "generated = \"\"\n",
    "\n",
    "# generate n_chars \n",
    "for i in tqdm.tqdm(range(n_chars), \"Generating text\"):\n",
    "\n",
    "    # make the input sequence\n",
    "    X = np.zeros((1, subsequence_length, vocab_size))\n",
    "    \n",
    "    for t, char in enumerate(seed):\n",
    "        X[0, (subsequence_length - len(seed)) + t, char_int[char]] = 1\n",
    "\n",
    "    # predict the next character\n",
    "    predicted = model.predict(X, verbose=0)[0]\n",
    "\n",
    "    # converting the vector to an integer\n",
    "    next_index = np.argmax(predicted)\n",
    "\n",
    "    # converting the integer to a character\n",
    "    next_char = int_char[next_index]\n",
    "\n",
    "    # add the character to results\n",
    "    generated += next_char\n",
    "\n",
    "    # shift seed and the predicted character\n",
    "    seed = seed[1:] + next_char\n",
    "\n",
    "print(f\"Seed: {s}\\n\")\n",
    "print(f\"Generated Text: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)\n",
    "\n",
    "Every once in a while it generates text pretty well! But sometimes it just comes out with gibberish. Most likely need more cereal slogan data to get a better result!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cereal-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
